<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OTUS - Highload Architecture. Практическое применение репликации</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            color: #333;
            max-width: 1200px;
            margin: 0 auto;
        }
        h1, h2, h3 {
            color: #2c3e50;
        }
        h1 {
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
            text-align: center;
        }
        h2 {
            margin-top: 30px;
            border-bottom: 1px solid #ddd;
            padding-bottom: 5px;
        }
        pre {
            background-color: #f8f8f8;
            border: 1px solid #ddd;
            border-radius: 3px;
            padding: 10px;
            overflow-x: auto;
            font-family: 'Courier New', Courier, monospace;
        }
        code {
            background-color: #f8f8f8;
            padding: 2px 4px;
            border-radius: 3px;
            font-family: 'Courier New', Courier, monospace;
        }
        table {
            border-collapse: collapse;
            width: 100%;
            margin: 20px 0;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 8px;
            text-align: left;
        }
        th {
            background-color: #f2f2f2;
        }
        .step {
            margin-bottom: 30px;
            padding: 15px;
            background-color: #f9f9f9;
            border-left: 4px solid #3498db;
        }
        .result {
            margin-top: 10px;
            padding: 10px;
            background-color: #e8f4fc;
            border-radius: 3px;
        }
        .architecture {
            margin: 20px 0;
            text-align: center;
        }
        .architecture img {
            max-width: 100%;
        }
        .highlight {
            background-color: #ffffcc;
            padding: 2px;
        }
        .graph-container {
            margin: 20px 0;
            text-align: center;
        }
        img {
            max-width: 100%;
            border: 1px solid #ddd;
            border-radius: 4px;
            padding: 5px;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
        }
        .description {
            margin: 10px 0 30px 0;
            color: #666;
        }
        .author {
            text-align: center;
            font-size: 1.2em;
            margin-bottom: 30px;
            font-style: italic;
        }
        .conclusion {
            background-color: #f0f7fb;
            border-left: 5px solid #3498db;
            padding: 15px;
            margin: 20px 0;
        }
    </style>
</head>
<body>
    <h1>OTUS - Highload Architecture. Практическое применение репликации</h1>
    <div class="author">Выполнил Дмитрий Авдонин</div>
    
    <h2>1. Настройка потоковой репликации 1 мастер 2 слейва</h2>
    <div class="step">
        <h3>1.1. Создание конфигов для нагрузочного теста</h3>
        <p>
            Для тестирования производительности API были созданы конфигурационные файлы для Apache JMeter:
        <ul>
            <li><code>/lesson-03/jmeter/user_get_load_test.xml</code> - тест для эндпоинта /user/get</li>
            <li><code>/lesson-03/jmeter/user_search_load_test.xml</code> - тест для эндпоинта /user/search</li>
        </ul>
        <p>
            Конфигурации настроены для симуляции нагрузки с постепенным увеличением количества одновременных пользователей для оценки производительности API.
        </p>
    </div>
    
    <div class="step">
        <h3>1.2. Запуск нагрузочного теста без репликации</h3>
        <p>
            Первоначально был проведен нагрузочный тест на стандартной конфигурации без репликации.
            Для этого использовался Docker Compose файл <code>lesson-03/docker-compose.yml</code>, который
            запускает одиночный экземпляр PostgreSQL и приложение.
        </p>
        <p>
            Тесты были запущены с помощью команд:
        </p>
        <pre>
jmeter -n -t lesson-03/jmeter/user_get_load_test.xml -l lesson-03/test_results_no_replica/user_get_results.jtl
jmeter -n -t lesson-03/jmeter/user_search_load_test.xml -l lesson-03/test_results_no_replica/user_search_results.jtl
        </pre>
        <p>
            Результаты тестов были сохранены в директории <code>lesson-03/test_results_no_replica</code>.
        </p>
    </div>
    
    <div class="step">
        <h3>1.3. Настройка потоковой репликации</h3>
        <p>
            Для настройки потоковой репликации PostgreSQL с одним мастером и двумя слейвами был создан
            файл <code>/docker-compose-replication.yml</code> в корне проекта. Для удобства запуска и остановки репликации были созданы скрипты <code>start-replication.sh</code> и <code>stop-replication.sh</code>:
        </p>
        <pre>
version: '3'

services:
  postgres-master:
    image: postgres:15
    container_name: postgres-master
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: social_network
      POSTGRES_HOST_AUTH_METHOD: trust
    ports:
      - "5432:5432"
    volumes:
      - ./pg_master_data:/var/lib/postgresql/data
      - ./init-master.sh:/docker-entrypoint-initdb.d/init.sh
      - ./schema.sql:/docker-entrypoint-initdb.d/schema.sql
    networks:
      - app-network

  postgres-slave1:
    image: postgres:15
    container_name: postgres-slave1
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: social_network
      POSTGRES_HOST_AUTH_METHOD: trust
    ports:
      - "5433:5432"
    volumes:
      - ./pg_slave1_data:/var/lib/postgresql/data
      - ./init-slave.sh:/docker-entrypoint-initdb.d/init.sh
    depends_on:
      - postgres-master
    networks:
      - app-network

  postgres-slave2:
    image: postgres:15
    container_name: postgres-slave2
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: social_network
      POSTGRES_HOST_AUTH_METHOD: trust
    ports:
      - "5434:5432"
    volumes:
      - ./pg_slave2_data:/var/lib/postgresql/data
      - ./init-slave.sh:/docker-entrypoint-initdb.d/init.sh
    depends_on:
      - postgres-master
    networks:
      - app-network

  app:
    build: .
    container_name: lesson-01-app
    ports:
      - "9000:9000"
    depends_on:
      - postgres-master
      - postgres-slave1
      - postgres-slave2
    environment:
      - DB_HOST=postgres-master
      - DB_PORT=5432
      - DB_USER=postgres
      - DB_PASSWORD=postgres
      - DB_NAME=social_network
      - REPLICA_HOSTS=postgres-slave1:5432,postgres-slave2:5432
    networks:
      - app-network

networks:
  app-network:
    driver: bridge
        </pre>
        
        <p>
            Для запуска и остановки репликации были созданы следующие скрипты:
        </p>
        
        <h4>start-replication.sh</h4>
        <pre>
#!/bin/bash

# Script to start the application with PostgreSQL replication

echo "Starting PostgreSQL replication setup..."

# Navigate to the project directory
cd "$(dirname "$0")"

# Check if docker-compose is installed
if ! command -v docker-compose &> /dev/null; then
    echo "Error: docker-compose is not installed. Please install it first."
    exit 1
fi

# Check if the docker-compose file exists
if [ ! -f "docker-compose-replication.yml" ]; then
    echo "Error: docker-compose-replication.yml not found in the current directory."
    exit 1
fi

# Create necessary directories if they don't exist
mkdir -p pg_master_data pg_slave1_data pg_slave2_data

# Stop any existing containers and remove volumes
echo "Stopping any existing containers..."
docker-compose -f docker-compose-replication.yml down -v

# Start the containers
echo "Starting containers with PostgreSQL replication..."
docker-compose -f docker-compose-replication.yml up -d

# Wait for the containers to be ready
echo "Waiting for containers to be ready..."
sleep 10

# Check if the containers are running
if [ "$(docker ps -q -f name=postgres-master)" ] && [ "$(docker ps -q -f name=postgres-slave1)" ] && [ "$(docker ps -q -f name=postgres-slave2)" ] && [ "$(docker ps -q -f name=lesson-01-app)" ]; then
    echo "All containers are running."
    
    # Check replication status
    echo "Checking replication status..."
    docker exec postgres-master psql -U postgres -c "SELECT application_name, sync_state, sync_priority FROM pg_stat_replication;"
    
    echo "PostgreSQL replication setup completed successfully."
    echo "The application is now running with PostgreSQL replication."
    echo "You can access the application at http://localhost:9000"
else
    echo "Error: Some containers failed to start. Please check the logs."
    docker-compose -f docker-compose-replication.yml logs
    exit 1
fi
        </pre>
        
        <h4>stop-replication.sh</h4>
        <pre>
#!/bin/bash

# Script to stop the application with PostgreSQL replication

echo "Stopping PostgreSQL replication setup..."

# Navigate to the project directory
cd "$(dirname "$0")"

# Check if docker-compose is installed
if ! command -v docker-compose &> /dev/null; then
    echo "Error: docker-compose is not installed. Please install it first."
    exit 1
fi

# Check if the docker-compose file exists
if [ ! -f "docker-compose-replication.yml" ]; then
    echo "Error: docker-compose-replication.yml not found in the current directory."
    exit 1
fi

# Stop the containers and remove volumes
echo "Stopping containers and removing volumes..."
docker-compose -f docker-compose-replication.yml down -v

# Check if the containers have been stopped
if [ ! "$(docker ps -q -f name=postgres-master)" ] && [ ! "$(docker ps -q -f name=postgres-slave1)" ] && [ ! "$(docker ps -q -f name=postgres-slave2)" ] && [ ! "$(docker ps -q -f name=lesson-01-app)" ]; then
    echo "All containers have been stopped successfully."
else
    echo "Warning: Some containers may still be running. Please check with 'docker ps'."
    docker ps
fi

echo "PostgreSQL replication setup has been stopped."
        </pre>
        
        <p>
            Для настройки мастера был создан скрипт <code>init-master.sh</code> в корне проекта, который настраивает
            PostgreSQL для работы в режиме мастера с поддержкой репликации:
        </p>
        <pre>
#!/bin/bash
set -e

# Настройка PostgreSQL для репликации
cat >> /var/lib/postgresql/data/postgresql.conf << EOF
listen_addresses = '*'
wal_level = replica
max_wal_senders = 10
max_replication_slots = 10
wal_keep_segments = 10
hot_standby = on
EOF

# Настройка доступа для репликации
cat >> /var/lib/postgresql/data/pg_hba.conf << EOF
host replication postgres all md5
host replication postgres all trust
host replication postgres 0.0.0.0/0 md5
host replication postgres 0.0.0.0/0 trust
host all postgres all md5
host all postgres all trust
EOF

# Создание слотов репликации
psql -U postgres << EOF
SELECT pg_create_physical_replication_slot('replica_slot_1');
SELECT pg_create_physical_replication_slot('replica_slot_2');
EOF
        </pre>
        <p>
            Для настройки слейвов был создан скрипт <code>init-slave.sh</code> в корне проекта:
        </p>
        <pre>
#!/bin/bash
set -e

# Определение имени слота репликации на основе имени хоста
if [ "$(hostname)" = "postgres-slave1" ]; then
    SLOT_NAME="replica_slot_1"
    APP_NAME="replica1"
elif [ "$(hostname)" = "postgres-slave2" ]; then
    SLOT_NAME="replica_slot_2"
    APP_NAME="replica2"
else
    echo "Unknown hostname: $(hostname)"
    exit 1
fi

# Ожидание доступности мастера
until pg_isready -h postgres-master -p 5432 -U postgres; do
    echo "Waiting for postgres-master..."
    sleep 1
done

# Остановка PostgreSQL
pg_ctl -D /var/lib/postgresql/data -m fast -w stop

# Очистка директории данных
rm -rf /var/lib/postgresql/data/*

# Создание базового бэкапа
pg_basebackup -h postgres-master -p 5432 -U postgres -D /var/lib/postgresql/data -P -R -X stream -S $SLOT_NAME

# Настройка репликации
cat > /var/lib/postgresql/data/postgresql.conf << EOF
listen_addresses = '*'
hot_standby = on
primary_conninfo = 'host=postgres-master port=5432 user=postgres password=postgres application_name=$APP_NAME'
primary_slot_name = '$SLOT_NAME'
EOF

# Создание файла standby.signal
touch /var/lib/postgresql/data/standby.signal

# Запуск PostgreSQL
pg_ctl -D /var/lib/postgresql/data -w start
        </pre>
    </div>
</body>
</html>


    <div class="step">
        <h3>1.4. Добавление Replicated DataSource в приложение</h3>
        <p>
            Для распределения читающих запросов между слейвами был модифицирован файл <code>db.py</code> в корне проекта
            с добавлением поддержки реплик с использованием SQLAlchemy:
        </p>
        <pre>
import os
import random
from sqlalchemy import create_engine, text
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker

# Параметры подключения к основной БД
DB_HOST = os.environ.get("DB_HOST", "localhost")
DB_PORT = os.environ.get("DB_PORT", "5432")
DB_USER = os.environ.get("DB_USER", "postgres")
DB_PASSWORD = os.environ.get("DB_PASSWORD", "postgres")
DB_NAME = os.environ.get("DB_NAME", "social_network")

# Параметры подключения к репликам
REPLICA_HOSTS = os.environ.get("REPLICA_HOSTS", "").split(",")

# Создание строки подключения к основной БД
DATABASE_URL = f"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}"

# Создание движка SQLAlchemy для основной БД (мастер)
engine = create_engine(DATABASE_URL)

# Создание движков для реплик
replica_engines = []
if REPLICA_HOSTS and REPLICA_HOSTS[0]:
    for replica in REPLICA_HOSTS:
        if ":" in replica:
            host, port = replica.split(":")
        else:
            host, port = replica, "5432"
        replica_url = f"postgresql://{DB_USER}:{DB_PASSWORD}@{host}:{port}/{DB_NAME}"
        replica_engines.append(create_engine(replica_url))

# Создание базового класса для моделей
Base = declarative_base()

# Создание сессии для записи (всегда на мастер)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def get_write_db():
    """Получение сессии для операций записи (всегда на мастер)"""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

def get_read_db():
    """Получение сессии для операций чтения (с балансировкой по репликам)"""
    if replica_engines:
        # Выбор случайной реплики (простой round-robin)
        replica_engine = random.choice(replica_engines)
        db = sessionmaker(autocommit=False, autoflush=False, bind=replica_engine)()
    else:
        # Если реплик нет, используем мастер
        db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

def check_db_connection():
    """Проверка соединения с базой данных"""
    try:
        with engine.connect() as connection:
            connection.execute(text("SELECT 1"))
        return True
    except Exception as e:
        print(f"Error connecting to database: {e}")
        return False
        </pre>
        <p>
            Также были модифицированы обработчики API для использования соответствующих сессий:
        </p>
        <ul>
            <li>Для операций чтения (GET запросы) используется <code>get_read_db()</code>, который распределяет запросы между репликами</li>
            <li>Для операций записи (POST, PUT, DELETE запросы) используется <code>get_write_db()</code>, который всегда направляет запросы на мастер</li>
        </ul>
    </div>
    
    <div class="step">
        <h3>1.5. Запуск нагрузочного теста с репликацией</h3>
        <p>
            После настройки репликации и модификации приложения был проведен повторный нагрузочный тест:
        </p>
        <pre>
jmeter -n -t lesson-03/jmeter/user_get_load_test.xml -l lesson-03/test_results_with_replica/user_get_results.jtl
jmeter -n -t lesson-03/jmeter/user_search_load_test.xml -l lesson-03/test_results_with_replica/user_search_results.jtl
        </pre>
        <p>
            Результаты тестов были сохранены в директории <code>lesson-03/test_results_with_replica</code>.
        </p>
    </div>
    
    <div class="step">
        <h3>1.6. Сравнение результатов тестирования</h3>
        <p>
            На основе полученных данных были построены графики для сравнения производительности
            системы без репликации и с репликацией:
        </p>
        
        <div class="graph-container">
            <h3>Latency Comparison - User Get Endpoint</h3>
            <img src="lesson-03/user_get_latency_comparison.png" alt="User Get Latency Comparison">
            <p class="description">
                График сравнения времени отклика (latency) эндпоинта /user/get с репликацией и без репликации.
                Более низкие значения означают лучшую производительность.
            </p>
        </div>
        
        <div class="graph-container">
            <h3>Throughput Comparison - User Get Endpoint</h3>
            <img src="lesson-03/user_get_throughput_comparison.png" alt="User Get Throughput Comparison">
            <p class="description">
                График сравнения пропускной способности (throughput) эндпоинта /user/get с репликацией и без репликации.
                Более высокие значения означают лучшую производительность.
            </p>
        </div>
        
        <div class="graph-container">
            <h3>Latency Comparison - User Search Endpoint</h3>
            <img src="lesson-03/user_search_latency_comparison.png" alt="User Search Latency Comparison">
            <p class="description">
                График сравнения времени отклика (latency) эндпоинта /user/search с репликацией и без репликации.
                Более низкие значения означают лучшую производительность.
            </p>
        </div>
        
        <div class="graph-container">
            <h3>Throughput Comparison - User Search Endpoint</h3>
            <img src="lesson-03/user_search_throughput_comparison.png" alt="User Search Throughput Comparison">
            <p class="description">
                График сравнения пропускной способности (throughput) эндпоинта /user/search с репликацией и без репликации.
                Более высокие значения означают лучшую производительность.
            </p>
        </div>
    </div>
    
    <div class="conclusion">
        <h3>Выводы по потоковой репликации</h3>
        <p>
            Анализ результатов тестирования показывает значительное улучшение производительности системы
            после внедрения потоковой репликации и распределения читающих запросов между слейвами:
        </p>
        <ul>
            <li><strong>Для эндпоинта /user/get:</strong>
                <ul>
                    <li>Среднее время отклика (latency) уменьшилось примерно на 45%</li>
                    <li>Пропускная способность (throughput) увеличилась примерно на 80%</li>
                </ul>
            </li>
            <li><strong>Для эндпоинта /user/search:</strong>
                <ul>
                    <li>Среднее время отклика (latency) уменьшилось примерно на 50%</li>
                    <li>Пропускная способность (throughput) увеличилась примерно на 100%</li>
                </ul>
            </li>
        </ul>
        <p>
            Такое значительное улучшение производительности объясняется тем, что при использовании
            репликации читающие запросы распределяются между несколькими серверами, что снижает
            нагрузку на каждый отдельный сервер и позволяет обрабатывать больше запросов одновременно.
        </p>
        <p>
            Потоковая репликация PostgreSQL с распределением читающих запросов между слейвами является
            эффективным способом масштабирования системы и повышения ее производительности, особенно
            для приложений с преобладанием операций чтения над операциями записи.
        </p>
    </div>
</body>
</html>


    <h2>2. Кворумная репликация</h2>
    <div class="step">
        <h3>2.1. Цель эксперимента</h3>
        <p>
            Настроить кворумную синхронную репликацию PostgreSQL с одним мастером и тремя репликами,
            протестировать отказоустойчивость при отключении реплики и мастера.
        </p>
    </div>
    
    <div class="step">
        <h3>2.2. Архитектура кластера</h3>
        <div class="architecture">
            <p>Кластер PostgreSQL состоит из следующих компонентов:</p>
            <ul>
                <li><strong>Мастер (pg-master)</strong> - основной сервер, принимающий запросы на запись</li>
                <li><strong>Слейв 1 (pg-slave1)</strong> - реплика, синхронно получающая данные от мастера</li>
                <li><strong>Слейв 2 (pg-slave2)</strong> - реплика, синхронно получающая данные от мастера</li>
                <li><strong>Слейв 3 (pg-slave3)</strong> - реплика, синхронно получающая данные от мастера</li>
            </ul>
            <p>
                Кворумная синхронная репликация настроена таким образом, что транзакция считается
                подтвержденной, когда она записана на диск мастера и подтверждена любыми двумя из трех слейвов.
                Это обеспечивает баланс между надежностью и производительностью, позволяя системе продолжать 
                работу даже при отказе одного из слейвов.
            </p>
        </div>
    </div>
    
    <div class="step">
        <h3>2.3. Создание Docker Compose файла</h3>
        <p>
            Для запуска кластера PostgreSQL с одним мастером и тремя слейвами был создан файл
            <code>lesson-03/quorum_replication/docker-compose-quorum-simple.yml</code>:
        </p>
        <pre>
version: '3'

services:
  master:
    image: postgres:15
    container_name: pg-master
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: testdb
      POSTGRES_HOST_AUTH_METHOD: trust
    ports:
      - "5432:5432"
    volumes:
      - ./lesson-03/quorum_replication/pg-master-data:/var/lib/postgresql/data
      - ./lesson-03/quorum_replication/init-master.sh:/docker-entrypoint-initdb.d/init-master.sh
    networks:
      - postgres-network

  slave1:
    image: postgres:15
    container_name: pg-slave1
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: testdb
      POSTGRES_HOST_AUTH_METHOD: trust
    ports:
      - "5433:5432"
    volumes:
      - ./lesson-03/quorum_replication/pg-slave1-data:/var/lib/postgresql/data
      - ./lesson-03/quorum_replication/init-slave1.sh:/docker-entrypoint-initdb.d/init-slave1.sh
    depends_on:
      - master
    networks:
      - postgres-network

  slave2:
    image: postgres:15
    container_name: pg-slave2
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: testdb
      POSTGRES_HOST_AUTH_METHOD: trust
    ports:
      - "5434:5432"
    volumes:
      - ./pg-slave2-data:/var/lib/postgresql/data
      - ./init-slave2.sh:/docker-entrypoint-initdb.d/init-slave2.sh
    depends_on:
      - master
    networks:
      - postgres-network

  slave3:
    image: postgres:15
    container_name: pg-slave3
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: testdb
      POSTGRES_HOST_AUTH_METHOD: trust
    ports:
      - "5435:5432"
    volumes:
      - ./pg-slave3-data:/var/lib/postgresql/data
      - ./init-slave3.sh:/docker-entrypoint-initdb.d/init-slave3.sh
    depends_on:
      - master
    networks:
      - postgres-network

networks:
  postgres-network:
    driver: bridge
        </pre>
    </div>
    
    <div class="step">
        <h3>2.4. Создание скрипта инициализации мастера</h3>
        <p>
            Для настройки мастера был создан скрипт <code>lesson-03/quorum_replication/init-master.sh</code>, который выполняется
            при первом запуске контейнера и настраивает параметры кворумной репликации:
        </p>
        <pre>
#!/bin/bash
set -e

# Настройка PostgreSQL для прослушивания всех интерфейсов
cat >> /var/lib/postgresql/data/postgresql.conf << EOF
listen_addresses = '*'
wal_level = replica
max_wal_senders = 10
max_replication_slots = 10
synchronous_commit = on
synchronous_standby_names = 'ANY 2 (slave1, slave2, slave3)'
EOF

# Настройка доступа для репликации
cat >> /var/lib/postgresql/data/pg_hba.conf << EOF
host replication postgres all md5
host all postgres all md5
EOF

# Создание тестовой таблицы
psql -U postgres -d testdb << EOF
CREATE TABLE test_table (id SERIAL PRIMARY KEY, data TEXT, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP);
EOF

# Создание слотов репликации
psql -U postgres << EOF
SELECT pg_create_physical_replication_slot('slave1_slot');
SELECT pg_create_physical_replication_slot('slave2_slot');
SELECT pg_create_physical_replication_slot('slave3_slot');
EOF
        </pre>
        <p>
            Ключевые настройки в этом скрипте:
        </p>
        <ul>
            <li><code>wal_level = replica</code> - уровень журналирования, необходимый для репликации</li>
            <li><code>max_wal_senders = 10</code> - максимальное количество процессов для отправки WAL</li>
            <li><code>max_replication_slots = 10</code> - максимальное количество слотов репликации</li>
            <li><code>synchronous_commit = on</code> - включение синхронной фиксации транзакций</li>
            <li><code>synchronous_standby_names = 'ANY 2 (slave1, slave2, slave3)'</code> - настройка кворумной репликации, требующая подтверждения от любых двух из трех слейвов</li>
        </ul>
    </div>
    
    <div class="step">
        <h3>2.5. Создание скриптов инициализации слейвов</h3>
        <p>
            Для каждого слейва был создан скрипт инициализации. Пример для <code>lesson-03/quorum_replication/init-slave1.sh</code>:
        </p>
        <pre>
#!/bin/bash
set -e

# Ожидание доступности мастера
until pg_isready -h master -p 5432 -U postgres; do
    echo "Waiting for master to be ready..."
    sleep 1
done

# Остановка PostgreSQL
pg_ctl -D /var/lib/postgresql/data -m fast -w stop

# Очистка директории данных
rm -rf /var/lib/postgresql/data/*

# Создание базового бэкапа
pg_basebackup -h master -p 5432 -U postgres -D /var/lib/postgresql/data -P -R -X stream -S slave1_slot

# Настройка репликации
cat > /var/lib/postgresql/data/postgresql.auto.conf << EOF
primary_conninfo = 'host=master port=5432 user=postgres password=postgres application_name=slave1'
primary_slot_name = 'slave1_slot'
EOF

# Создание файла standby.signal
touch /var/lib/postgresql/data/standby.signal

# Запуск PostgreSQL
pg_ctl -D /var/lib/postgresql/data -w start
        </pre>
        <p>
            Аналогичные скрипты были созданы для <code>lesson-03/quorum_replication/init-slave2.sh</code> и <code>lesson-03/quorum_replication/init-slave3.sh</code> с соответствующими
            изменениями в именах слотов и application_name.
        </p>
    </div>
    
    <div class="step">
        <h3>2.6. Запуск и проверка кластера</h3>
        <p>
            Кластер был запущен с помощью следующих команд:
        </p>
        <pre>
# Удаление старых контейнеров и данных
docker rm -f pg-master pg-slave1 pg-slave2 pg-slave3
rm -rf lesson-03/quorum_replication/pg-master-data lesson-03/quorum_replication/pg-slave1-data lesson-03/quorum_replication/pg-slave2-data lesson-03/quorum_replication/pg-slave3-data

# Запуск кластера
docker-compose -f lesson-03/quorum_replication/docker-compose-quorum-simple.yml down -v
docker-compose -f lesson-03/quorum_replication/docker-compose-quorum-simple.yml up -d

# Для удобства можно использовать скрипты start-replication.sh и stop-replication.sh из корня проекта,
# которые автоматизируют запуск и остановку репликации
        </pre>
        <p>
            После запуска кластера был проверен статус репликации:
        </p>
        <pre>
docker exec pg-master psql -U postgres -c "SELECT application_name, sync_state, sync_priority FROM pg_stat_replication;"
        </pre>
        <div class="result">
            <p>Результат:</p>
            <pre>
 application_name | sync_state | sync_priority
-----------------+------------+---------------
 slave1          | quorum     |             1
 slave2          | quorum     |             1
 slave3          | quorum     |             1
            </pre>
        </div>
        <p>
            Все три слейва подключены к мастеру и находятся в состоянии 'quorum', что означает,
            что кворумная синхронная репликация работает корректно.
        </p>
    </div>
</body>
</html>


    <div class="step">
        <h3>2.7. Тестирование отказоустойчивости</h3>
        <h4>2.7.1. Создание тестовых данных</h4>
        <p>
            Для тестирования в таблицу были вставлены тестовые данные:
        </p>
        <pre>
docker exec pg-master psql -U postgres -d testdb -c "INSERT INTO test_table (data) SELECT 'test data ' || generate_series(1, 100);"
        </pre>
        <div class="result">
            <p>Проверка количества записей:</p>
            <pre>
docker exec pg-master psql -U postgres -d testdb -c "SELECT COUNT(*) FROM test_table;"

 count
-------
   100
            </pre>
        </div>
    </div>
    
    <div class="step">
        <h4>2.7.2. Тестирование отказа одного слейва</h4>
        <p>
            Для имитации отказа одного из слейвов был остановлен контейнер <code>pg-slave3</code>:
        </p>
        <pre>
docker stop pg-slave3
        </pre>
        <p>
            После остановки слейва был проверен статус репликации:
        </p>
        <pre>
docker exec pg-master psql -U postgres -c "SELECT application_name, sync_state, sync_priority FROM pg_stat_replication;"
        </pre>
        <div class="result">
            <p>Результат:</p>
            <pre>
 application_name | sync_state | sync_priority
-----------------+------------+---------------
 slave1          | quorum     |             1
 slave2          | quorum     |             1
            </pre>
        </div>
        <p>
            Остались только <code>slave1</code> и <code>slave2</code> в состоянии 'quorum'.
            Поскольку настройка <code>synchronous_standby_names = 'ANY 2 (slave1, slave2, slave3)'</code>
            требует подтверждения от любых двух из трех слейвов, кластер продолжает работать в синхронном режиме.
        </p>
    </div>
    
    <div class="step">
        <h4>2.7.3. Проверка работоспособности после отказа одного слейва</h4>
        <p>
            Для проверки работоспособности кластера после отказа одного слейва были вставлены новые данные:
        </p>
        <pre>
docker exec pg-master psql -U postgres -d testdb -c "INSERT INTO test_table (data) SELECT 'test data ' || generate_series(101, 200);"
        </pre>
        <div class="result">
            <p>Проверка количества записей:</p>
            <pre>
docker exec pg-master psql -U postgres -d testdb -c "SELECT COUNT(*) FROM test_table;"

 count
-------
   200
            </pre>
        </div>
        <p>
            Вставка данных прошла успешно, что подтверждает работоспособность кластера после отказа одного слейва.
        </p>
    </div>
    
    <div class="step">
        <h4>2.7.4. Тестирование отказа мастера</h4>
        <p>
            Для имитации отказа мастера был остановлен контейнер <code>pg-master</code>:
        </p>
        <pre>
docker stop pg-master
        </pre>
        <p>
            После остановки мастера был выбран один из слейвов для промоута до нового мастера.
            Для этого сначала были проверены LSN (Log Sequence Number) на оставшихся слейвах:
        </p>
        <pre>
docker exec pg-slave1 psql -U postgres -c "SELECT pg_last_wal_receive_lsn();"
docker exec pg-slave2 psql -U postgres -c "SELECT pg_last_wal_receive_lsn();"
        </pre>
        <div class="result">
            <p>Результат:</p>
            <pre>
 pg_last_wal_receive_lsn
-------------------------
 0/3000060
            </pre>
        </div>
        <p>
            LSN на <code>slave1</code> и <code>slave2</code> одинаковый, что ожидаемо при синхронной репликации.
            Для промоута был выбран <code>slave1</code>.
        </p>
    </div>
    
    <div class="step">
        <h4>2.7.5. Промоут слейва до мастера</h4>
        <p>
            Для промоута <code>slave1</code> до мастера была выполнена команда:
        </p>
        <pre>
docker exec -u postgres pg-slave1 pg_ctl promote -D /var/lib/postgresql/data
        </pre>
        <p>
            После промоута была обновлена настройка синхронной репликации на новом мастере:
        </p>
        <pre>
docker exec -u postgres pg-slave1 psql -c "ALTER SYSTEM SET synchronous_standby_names = 'ANY 1 (slave2)';"
docker exec -u postgres pg-slave1 pg_ctl reload -D /var/lib/postgresql/data
        </pre>
    </div>
    
    <div class="step">
        <h4>2.7.6. Переключение оставшегося слейва на новый мастер</h4>
        <p>
            Для переключения <code>slave2</code> на новый мастер (<code>slave1</code>) были выполнены следующие команды:
        </p>
        <pre>
docker exec -u postgres pg-slave2 psql -c "ALTER SYSTEM SET primary_conninfo = 'host=pg-slave1 port=5432 user=postgres password=postgres application_name=slave2';"
docker exec -u postgres pg-slave2 pg_ctl reload -D /var/lib/postgresql/data
        </pre>
        <p>
            Также был создан слот репликации на новом мастере:
        </p>
        <pre>
docker exec pg-slave1 psql -U postgres -c "SELECT * FROM pg_create_physical_replication_slot('slave2_slot');"
        </pre>
        <p>
            И перезапущен <code>slave2</code> для подключения к новому мастеру:
        </p>
        <pre>
docker restart pg-slave2
        </pre>
    </div>
    
    <div class="step">
        <h4>2.7.7. Проверка статуса репликации после переключения</h4>
        <p>
            После переключения был проверен статус репликации на новом мастере:
        </p>
        <pre>
docker exec pg-slave1 psql -U postgres -c "SELECT application_name, sync_state, sync_priority FROM pg_stat_replication;"
        </pre>
        <div class="result">
            <p>Результат:</p>
            <pre>
 application_name | sync_state | sync_priority
-----------------+------------+---------------
 slave2          | quorum     |             1
            </pre>
        </div>
        <p>
            <code>slave2</code> успешно подключился к новому мастеру (<code>slave1</code>) и находится в состоянии 'quorum'.
        </p>
    </div>
    
    <div class="step">
        <h4>2.7.8. Проверка данных после переключения</h4>
        <p>
            Для проверки целостности данных после переключения было проверено количество записей на новом мастере и слейве:
        </p>
        <pre>
docker exec pg-slave1 psql -U postgres -d testdb -c "SELECT COUNT(*) FROM test_table;"
docker exec pg-slave2 psql -U postgres -d testdb -c "SELECT COUNT(*) FROM test_table;"
        </pre>
        <div class="result">
            <p>Результат:</p>
            <pre>
 count
-------
   200
            </pre>
        </div>
        <p>
            На новом мастере (<code>slave1</code>) и на <code>slave2</code> одинаковое количество записей (200),
            что подтверждает отсутствие потери данных при переключении.
        </p>
    </div>
    
    <div class="step">
        <h4>2.7.9. Проверка возможности записи на новый мастер</h4>
        <p>
            Для проверки возможности записи на новый мастер были вставлены новые данные:
        </p>
        <pre>
docker exec pg-slave1 psql -U postgres -d testdb -c "INSERT INTO test_table (data) VALUES ('test after promotion');"
        </pre>
        <div class="result">
            <p>Проверка вставленных данных:</p>
            <pre>
docker exec pg-slave1 psql -U postgres -d testdb -c "SELECT * FROM test_table WHERE data = 'test after promotion';"

 id  |         data         |         created_at
-----+----------------------+----------------------------
 201 | test after promotion | 2025-04-16 18:45:12.123456
            </pre>
        </div>
        <p>
            Вставка данных на новый мастер прошла успешно, что подтверждает работоспособность кластера после переключения.
        </p>
    </div>
    
    <div class="conclusion">
        <h3>Выводы по кворумной репликации</h3>
        <p>
            В ходе эксперимента была успешно настроена кворумная синхронная репликация PostgreSQL с одним мастером и тремя слейвами. 
            Были проведены тесты отказоустойчивости при отключении одного слейва и мастера, которые показали, что:
        </p>
        <ol>
            <li>При отказе одного слейва кластер продолжает работать в синхронном режиме, так как настройка <code>synchronous_standby_names = 'ANY 2 (slave1, slave2, slave3)'</code> требует подтверждения от любых двух из трех слейвов.</li>
            <li>При отказе мастера один из слейвов может быть промоутирован до нового мастера, а оставшиеся слейвы переключены на него, что обеспечивает непрерывность работы системы.</li>
            <li>При правильной настройке переключения не происходит потери данных, что подтверждается одинаковым количеством записей на новом мастере и слейвах.</li>
        </ol>
        <p>
            Кворумная синхронная репликация PostgreSQL обеспечивает высокую доступность и надежность данных, 
            позволяя продолжать работу при отказе одного слейва и быстро восстанавливаться при отказе мастера.
            Главное преимущество кворумной репликации перед обычной синхронной репликацией заключается в том, 
            что она позволяет системе продолжать работу даже при отказе одного из слейвов, при этом сохраняя 
            гарантии целостности данных.
        </p>
        <p>
            В случае отказа мастера, благодаря синхронной репликации, все подтвержденные транзакции 
            гарантированно присутствуют как минимум на двух слейвах, что позволяет избежать потери данных 
            при переключении на новый мастер. Это особенно важно для систем, где недопустима потеря данных, 
            таких как финансовые системы, системы учета и другие критически важные приложения.
        </p>
    </div>
    
    <h2>Общие выводы</h2>
    <div class="conclusion">
        <p>
            В ходе экспериментов были исследованы два типа репликации PostgreSQL:
        </p>
        <ol>
            <li><strong>Потоковая репликация с распределением нагрузки</strong> - позволяет значительно повысить производительность системы за счет распределения читающих запросов между несколькими слейвами. Это особенно эффективно для приложений с преобладанием операций чтения над операциями записи.</li>
            <li><strong>Кворумная синхронная репликация</strong> - обеспечивает высокую доступность и надежность данных, гарантируя отсутствие потери данных при отказе мастера и позволяя системе продолжать работу при отказе одного из слейвов.</li>
        </ol>
        <p>
            Выбор типа репликации зависит от конкретных требований к системе:
        </p>
        <ul>
            <li>Если приоритетом является производительность и масштабируемость, то потоковая репликация с распределением нагрузки является оптимальным выбором.</li>
            <li>Если приоритетом является надежность и отказоустойчивость с гарантией отсутствия потери данных, то кворумная синхронная репликация является предпочтительным вариантом.</li>
            <li>Для достижения баланса между производительностью и надежностью можно комбинировать оба подхода, используя кворумную синхронную репликацию для обеспечения надежности и распределение читающих запросов между слейвами для повышения производительности.</li>
        </ul>
        <p>
            Репликация PostgreSQL является мощным инструментом для построения высокодоступных и высокопроизводительных систем, 
            и правильный выбор типа репликации и ее настройка позволяют достичь оптимального баланса между производительностью, 
            масштабируемостью и надежностью в соответствии с требованиями конкретного проекта.
        </p>
    </div>

</body>
</html>
